{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Talk to your data with RAG and Llama 3.2\n",
    "\n",
    "In this notebook, you will learn how to use RAG and Llama 3.2 to talk to your data. Llama 3.2 is chosen because of its smaller size and faster speed compared to the original Llama. This allows us to run the code locally. RAG allows the model to generate text that is factually accurate and coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18426/693793077.py:10: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/tmp/ipykernel_18426/693793077.py:10: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()\n",
      "/home/alex/anaconda3/envs/llama-rag/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/alex/anaconda3/envs/llama-rag/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the paper\n",
    "loader = PyMuPDFLoader(\"~/Downloads/2410.05258v1.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Create embeddings\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create a vector store\n",
    "db = Chroma.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to augment your own knowledge. The context may not have all the information needed to answer the question, so use your own knowledge to provide a complete answer.\"),\n",
    "    (\"human\", \"Context: {context}\"),\n",
    "    (\"human\", \"Question: {input}\"),\n",
    "    (\"human\", \"Please provide a detailed answer, combining information from the context (if relevant) and your own knowledge.\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "llm_default = Ollama(model=\"llama3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the differential transformer differ from a traditional transformer?\n",
      "\n",
      " ** WITH CONTEXT **\n",
      "\n",
      "Answer: Here's a detailed explanation of how the Differential Transformer differs from a traditional Transformer:\n",
      "\n",
      "**Traditional Transformer**\n",
      "\n",
      "A traditional Transformer is a neural network architecture introduced in 2017 by Vaswani et al. [1] for natural language processing tasks, such as machine translation and text generation. The core idea behind the Transformer is to use self-attention mechanisms instead of convolutional layers to process sequential data.\n",
      "\n",
      "In a traditional Transformer, the input sequence is divided into overlapping windows of fixed size, which are then processed using self-attention mechanisms. This allows the model to attend to all positions in the sequence simultaneously and weigh their importance relative to each other.\n",
      "\n",
      "**Differential Transformer**\n",
      "\n",
      "The Differential Transformer is an extension of the original Transformer architecture that incorporates additional components to improve its performance on certain tasks, particularly those involving longer sequences.\n",
      "\n",
      "The main difference between the Differential Transformer and the traditional Transformer lies in its use of a learned scaling factor (α) that adapts to the sequence length. This scaling factor is applied to the attention weights and queries, allowing the model to focus more on longer-range dependencies.\n",
      "\n",
      "In addition to this scaling factor, the Differential Transformer uses an additional layer (denoted as \"diffusion\" layers in the paper [2]) that combines multiple scaled attention weights using a combination of addition, multiplication, and exponentiation. This allows the model to learn more complex relationships between different positions in the sequence.\n",
      "\n",
      "**Key differences**\n",
      "\n",
      "To summarize, the main differences between the Differential Transformer and the traditional Transformer are:\n",
      "\n",
      "1. **Scaling factor**: The Differential Transformer uses an additional learned scaling factor that adapts to the sequence length.\n",
      "2. **Additional layer**: The Differential Transformer has an additional layer that combines multiple scaled attention weights using a combination of mathematical operations.\n",
      "3. **Focus on longer-range dependencies**: The use of the scaling factor and additional layer allows the model to focus more on longer-range dependencies, which is particularly useful for tasks involving longer sequences.\n",
      "\n",
      "**Advantages**\n",
      "\n",
      "The Differential Transformer offers several advantages over traditional Transformers, including:\n",
      "\n",
      "1. **Improved performance on longer sequences**: By adapting to the sequence length through the learned scaling factor, the Differential Transformer can perform better on tasks that require longer sequences.\n",
      "2. **Increased flexibility**: The additional layer allows the model to learn more complex relationships between different positions in the sequence.\n",
      "\n",
      "However, it's worth noting that the Differential Transformer also introduces some additional computational overhead and training requirements due to the need for learning the scaling factor and additional layer weights.\n",
      "\n",
      "I hope this explanation helps! Let me know if you have any further questions.\n",
      "\n",
      "Sources:\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 17, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 15, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 4, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 3, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "\n",
      "\n",
      "\n",
      "** WITHOUT CONTEXT **\n",
      "\n",
      "A differential transformer is similar to a traditional transformer but has a few key differences:\n",
      "\n",
      "1. **Input/Output Configuration:** In a traditional transformer, the primary and secondary coils are connected in series or parallel. In contrast, a differential transformer has the primary and secondary coils connected in a way that one coil's current flows through one side of the core, while the other coil's current flows through the other side.\n",
      "\n",
      "2. **Current Flow:** As mentioned earlier, the primary and secondary coils are connected differently in a differential transformer. This affects how the currents in the coils interact with each other when a voltage is applied across them.\n",
      "\n",
      "3. **Transformation Ratio:** In traditional transformers, the transformation ratio (primary turns / secondary turns) determines how much voltage is transformed from one side to the other. However, the relationship between primary and secondary voltages in differential transformers depends on the difference in current flows rather than just a fixed transformation ratio.\n",
      "\n",
      "4. **Common Mode Voltage:** Due to its unique connection configuration, differential transformers are better suited for applications involving common-mode signals (signals present in both channels). Traditional transformers might not perform well with such signals due to their non-isolated design.\n",
      "\n",
      "5. **Signal Isolation:** While differential transformers do provide some isolation between the primary and secondary sides (in terms of current), they don't offer complete isolation like traditional transformer designs that use completely separate windings for each side.\n",
      "\n",
      "Differential transformers are particularly useful in applications where the signal is not common-mode and where a high degree of voltage transformation is required. However, their design offers some limitations compared to more conventional transformer designs.\n"
     ]
    }
   ],
   "source": [
    "def ask_question(chain, question):\n",
    "    result = chain.invoke({\"input\": question})\n",
    "    print(\"Question:\", question)\n",
    "    print(\"\\n ** WITH CONTEXT **\\n\")\n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"\\nSources:\")\n",
    "    for doc in result['context']:\n",
    "        print(doc.metadata)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    default_result = llm_default.invoke(question)\n",
    "    print(\"\\n** WITHOUT CONTEXT **\\n\")\n",
    "    print(default_result)\n",
    "\n",
    "question = \"How does the differential transformer differ from a traditional transformer?\"\n",
    "ask_question(rag_chain, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the query a few times and analyze the results. With or without context, the quality of the generated text is extremely inconsistent. If the purpose of this agent is to act as a study guide, there are certain pieces of information we would always like it provide in the response.\n",
    "\n",
    "- A general definition or answer to the question\n",
    "- A specific example\n",
    "- A related fact or piece of information\n",
    "\n",
    "This is something we might be able to inject in the model as a prompt. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to augment your own knowledge. The context may not have all the information needed to answer the question, so use your own knowledge to provide a complete answer. Your answer should always include the following: 1. A general definition or answer to the question. 2. A specific example. This includes code snippets either from the context provided or your own knowledge. 3. A related fact or piece of information.\"),\n",
    "    (\"human\", \"Context: {context}\"),\n",
    "    (\"human\", \"Question: {input}\"),\n",
    "    (\"human\", \"Please provide a detailed answer, combining information from the context (if relevant) and your own knowledge.\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the differential transformer differ from a traditional transformer?\n",
      "\n",
      " ** WITH CONTEXT **\n",
      "\n",
      "Answer: I'll do my best to provide a detailed answer.\n",
      "\n",
      "The Differential Transformer is a novel architecture for natural language processing tasks that differs significantly from traditional Transformers in several key ways. Here's a breakdown of the main differences:\n",
      "\n",
      "1. **Differential Attention Mechanism**: The most distinctive feature of Differential Transformer is its differential attention mechanism, which modifies the standard self-attention mechanism used in traditional Transformers. In traditional Transformers, all tokens are equally important and interact with each other through self-attention. In contrast, the Differential Transformer introduces a new attention mechanism that takes into account the relative importance of different tokens. This is achieved by learning a set of differential weights (W G, W1) for each token, which are used to compute the attention weights.\n",
      "\n",
      "2. **Contextualized Weights**: The differential attention mechanism also involves contextually weighted tokens. Specifically, each token's weight is computed as the dot product of its position embedding and learned differential weight (XW G). This allows the model to capture subtle differences in the importance of different tokens, especially when they are close together.\n",
      "\n",
      "3. **Differentiable Learning**: The Differential Transformer introduces a differentiable learning process, which involves optimizing the differential weights using a specific loss function. This approach enables the model to learn more efficient and effective attention mechanisms.\n",
      "\n",
      "4. **Less Computing FLOPs**: One of the key benefits of the Differential Transformer is its reduced computational cost compared to traditional Transformers. By introducing differential attention mechanisms and contextually weighted tokens, the model can reduce the number of computations required for attention operations.\n",
      "\n",
      "5. **Efficient Model Size**: The Differential Transformer also achieves significant reductions in model size while maintaining similar performance levels to traditional Transformers. This is achieved by using learned differential weights instead of static attention weights.\n",
      "\n",
      "Overall, the Differential Transformer represents a significant departure from traditional Transformers and offers several advantages, including reduced computing FLOPs, more efficient learning processes, and improved performance on certain tasks.\n",
      "\n",
      "Sources:\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 17, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 15, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 4, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "{'author': '', 'creationDate': 'D:20241008020722Z', 'creator': 'LaTeX with hyperref', 'file_path': '/home/alex/Downloads/2410.05258v1.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20241008020722Z', 'page': 3, 'producer': 'pdfTeX-1.40.25', 'source': '/home/alex/Downloads/2410.05258v1.pdf', 'subject': '', 'title': '', 'total_pages': 19, 'trapped': ''}\n",
      "\n",
      "\n",
      "\n",
      "** WITHOUT CONTEXT **\n",
      "\n",
      "The differential transformer, also known as a differential amplifier or differential transformer amplifier, differs from a traditional transformer in several ways:\n",
      "\n",
      "1. **Input connections**: In a traditional transformer, both primary and secondary windings are connected to power sources. In contrast, a differential transformer has the primary winding connected directly to one of the input signals (usually the signal source), and the secondary winding is connected to ground or a reference point.\n",
      "2. **Input impedance**: The differential transformer has a high input impedance due to its design, making it suitable for low-level signal applications. In contrast, traditional transformers typically have lower input impedance.\n",
      "3. **Output configuration**: A traditional transformer usually has an active output configuration, where the secondary winding is driven by the primary winding through a voltage-controlled current source (VCCS) or a transformer with a feedback loop. In contrast, the differential transformer has a passive output configuration, where the secondary winding is directly connected to ground.\n",
      "4. **Noise rejection**: The differential transformer's design provides better noise rejection compared to traditional transformers due to its high input impedance and low-frequency response characteristics.\n",
      "5. **Linearity**: Differential transformers are designed to provide linearity over a wide range of frequencies, making them suitable for applications requiring accurate signal reproduction. Traditional transformers may exhibit non-linearity in certain frequency ranges.\n",
      "6. **Transformer ratio**: The differential transformer typically has a fixed transformer ratio, whereas traditional transformers can have variable or adjustable ratios.\n",
      "\n",
      "The differential transformer is often used in audio equipment, instrumentation, and measurement systems where low-level signals require high accuracy and noise rejection.\n"
     ]
    }
   ],
   "source": [
    "question = \"How does the differential transformer differ from a traditional transformer?\"\n",
    "ask_question(rag_chain, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above testing, our RAG agent is beginning to produce more consistent results. One that that has been consistent is that it always pulls the correct notes. Another way of enhancing the responses lies in *query translation*. The first approach we will try is called **multi-query**. This will generate multiple questions based on the user's initial question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query\n",
    "template = \"\"\"Your task is to generate five different versions of the given user question to retrieve relevant documents from a vector database.\n",
    "Using the different perspectives from the retrieved documents, you should generate a response to the user question. Original question: {question}\"\"\"\n",
    "prompt_multi_query = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_multi_query\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18426/3490488652.py:23: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Differential Transformer (DIFF) is a proposed architecture that differs from the traditional Transformer in several key ways. Here's a detailed comparison:\n",
      "\n",
      "**1. Multi-needle retrieval protocol:** DIFF uses a multi-needle evaluation protocol, where multiple needles are inserted into varying depths within contexts of different lengths. Each needle consists of a concise sentence that assigns a unique magic number to a specific city. This is distinct from the traditional Transformer, which typically uses a single query and key.\n",
      "\n",
      "**2. Distraction noise:** DIFF introduces distraction noise by placing other distracting needles randomly, while maintaining a constant depth and length for the answer needle. This simulates real-world scenarios where relevant information may be surrounded by irrelevant noise. In contrast, traditional Transformers do not use distraction noise in their evaluation protocols.\n",
      "\n",
      "**3. Multi-needle placement:** The DIFF architecture evaluates 50 samples for each combination of depth and length. Each sample includes a different placement of the answer needle, while keeping other distracting needles constant. This allows for a more comprehensive evaluation of the model's performance under varying conditions.\n",
      "\n",
      "**4. Performance comparison:** The results show that both traditional Transformers and DIFF architectures perform well on tasks like multi-needle retrieval. However, DIFF maintains a consistent accuracy as the number of needles (N) and query cities (R) increases, whereas traditional Transformer's performance drops significantly with increased N and R.\n",
      "\n",
      "**5. Attention mechanism:** DIFF allocates higher attention scores to the answer span and has lower attention noise compared to traditional Transformers. This suggests that DIFF is better at preserving useful information against distractions in the input sequence.\n",
      "\n",
      "**6. Context length:** The DIFF architecture demonstrates stable performance across different context lengths, whereas traditional Transformer's average accuracy gradually declines as the context length increases up to the maximal length (64K).\n",
      "\n",
      "**7. Depth and position of key information:** When key information is inserted at different depths within the context, DIFF achieves higher accuracy improvements over traditional Transformer, particularly when needles are placed at shallow depths (25% and 50%).\n",
      "\n",
      "In summary, DIFF differs from traditional Transformers in its:\n",
      "\n",
      "* Multi-needle retrieval protocol\n",
      "* Introduction of distraction noise to simulate real-world scenarios\n",
      "* Evaluation protocol that includes multiple placements of answer needles\n",
      "* Better performance under varying conditions\n",
      "* Improved attention mechanism\n",
      "* Stable performance across different context lengths\n",
      "\n",
      "These differences suggest that DIFF is better suited for tasks requiring robustness against distractions and preserving relevant information in input sequences.\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"You are a helpful AI oracle used to assist students in studying.\n",
    "Use the following pieces of retrieved context along with your own knowledge to provide thorough answers to the user's questions.\n",
    "The context may not have all the information needed to answer the question, so use your own knowledge to provide a complete answer.\n",
    "Your answer should always include the following:\n",
    "# Summary\n",
    "A general definition or answer to the question.\n",
    "\n",
    "# Example\n",
    "A specific example from the context, including code examples. If no examples are provided in the context, you can use your own knowledge to provide an example.\n",
    "\n",
    "# Related Information\n",
    "A related fact or piece of information.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer, combining information from the context (if relevant) and your own knowledge.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = final_rag_chain.invoke({\"question\": question})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG-Fusion\n",
    "\n",
    "Generating multiple queries did not seem to enhance the quality of the model's reponses in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** WITH CONTEXT **\n",
      "The Differential Transformer (DIFF Transformer) differs from a traditional Transformer in its attention mechanism. In a traditional Transformer, the attention mechanism is based on softmax functions to compute attention scores between query, key, and value vectors. However, this mechanism can be noisy due to the presence of noise in the attention scores.\n",
      "\n",
      "In contrast, the DIFF Transformer uses a differential attention mechanism to cancel out the noise in the attention scores. This mechanism involves computing two separate softmax attention maps for two groups of query and key vectors, and then subtracting these two maps to obtain the final attention scores.\n",
      "\n",
      "The main difference between DIFF Transformer and traditional Transformer can be summarized as follows:\n",
      "\n",
      "1. **Attention Mechanism**: Traditional Transformer uses a single softmax function to compute attention scores, while DIFF Transformer uses two separate softmax functions to cancel out noise in the attention scores.\n",
      "2. **Noise Cancellation**: The DIFF Transformer's differential attention mechanism eliminates attention noise by subtracting the two softmax attention maps, which reduces the impact of irrelevant context on the model's performance.\n",
      "3. **Improved Focus**: By cancelling out noise in the attention scores, the DIFF Transformer encourages the model to focus on critical information and ignore irrelevant context.\n",
      "\n",
      "Overall, the DIFF Transformer is designed to improve the robustness and efficiency of sequence modeling tasks by reducing the impact of noisy attention mechanisms.\n",
      "\n",
      "Additionally, the DIFF Transformer also differs from traditional Transformer in its architecture. It uses a decoder-only model with stacked L DIFF Transformer layers, whereas traditional Transformer can be either decoder-only or encoder-decoder models.\n",
      "\n",
      "In terms of specific design choices, the DIFF Transformer:\n",
      "\n",
      "* Uses pre-RMSNorm and SwiGLU to improve the learning dynamics\n",
      "* Employs a differential attention operator (DiffAttn) that computes outputs based on query, key, value vectors, and a learnable scalar λ\n",
      "\n",
      "These design choices aim to mitigate the limitations of traditional Transformers, such as handling large sequence lengths and improving robustness to noisy attention mechanisms.\n",
      "\n",
      "** WITHOUT CONTEXT **\n",
      "\n",
      "A differential transformer differs from a traditional transformer in several key ways:\n",
      "\n",
      "1. **Differential winding**: The primary and secondary windings of a differential transformer are designed such that when a current flows through one winding, it creates a magnetic field that opposes the flow of current in the other winding. This results in a differential voltage being induced between the two windings.\n",
      "2. **No neutral point**: Unlike traditional transformers, which have a neutral point where the primary and secondary currents add up to zero, differential transformers do not have a neutral point. The absence of a neutral point means that both windings must be energized with current to induce a voltage in each winding.\n",
      "3. **Higher turns ratio**: Differential transformers typically require higher turns ratios (e.g., 10:1 or higher) than traditional transformers because the differential winding configuration requires more turns to achieve the same level of isolation between the primary and secondary circuits.\n",
      "4. **Increased isolation**: The differential transformer's unique design provides increased electrical isolation between the primary and secondary circuits, making it suitable for applications where high voltage and current levels are present.\n",
      "5. **Voltage regulation**: Differential transformers have a different voltage regulation characteristic compared to traditional transformers. Because both windings must be energized with current, differential transformers exhibit a more linear voltage regulation over a wider range of load currents.\n",
      "6. **Design complexity**: The differential winding configuration adds design complexity to the transformer, which can affect its overall efficiency and reliability.\n",
      "\n",
      "Differential transformers are often used in applications such as:\n",
      "\n",
      "* High-voltage isolation\n",
      "* Radio frequency (RF) transmission and reception\n",
      "* Audio and video equipment with high-impedance inputs\n",
      "* Medical devices requiring high-isolation transformer designs\n"
     ]
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"You are a helpful AI oracle used to assist students in studying.\n",
    "Use the following pieces of retrieved context along with your own knowledge to provide thorough answers to the user's questions.\n",
    "The context may not have all the information needed to answer the question, so use your own knowledge to provide a complete answer.\n",
    "Your answer should always include the following:\n",
    "# Summary\n",
    "A general definition or answer to the question.\n",
    "\n",
    "# Example\n",
    "A specific example from the context, including code examples. If no examples are provided in the context, you can use your own knowledge to provide an example.\n",
    "\n",
    "# Related Information\n",
    "A related fact or piece of information.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer, combining information from the context (if relevant) and your own knowledge.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"** WITH CONTEXT **\")\n",
    "result = final_rag_chain.invoke({\"question\":question})\n",
    "print(result)\n",
    "\n",
    "print(\"\\n** WITHOUT CONTEXT **\\n\")\n",
    "result = llm.invoke(question)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
